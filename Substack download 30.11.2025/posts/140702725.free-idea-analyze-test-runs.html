<p>What could test runs tell us about our software development process?</p><blockquote><p>I have too many ideas to implement, so I throw some out to you paying subscribers from time to time. I’ll try not to swamp you, but some just seem so good that I don’t want them to just wither.</p></blockquote><p>Here’s the idea—collect the results of all test runs, the runtime &amp; result of each test &amp; then of the whole test suite. Aggregate this across a large community, at least a team, better a whole company (my vision was to aggregate all test runs everywhere in the world, but that didn’t work out).</p><p>Now, sprinkle magic AI fairy dust on an analyzer. What does all that history tell us?</p><ul><li><p>Where are tests costing more than the value they create?</p></li><li><p>Can we infer holes in our tests?</p></li><li><p>Where are tests redundant?</p></li><li><p>How can we get the same confidence at less cost (in time or energy)?</p></li></ul><p>Now assume a worldwide repository of all test runs anywhere ever. What questions could we ask of that data? How would we address the privacy concerns?</p><p>But the first step is to gather a day’s worth of test runs, load it into a model, &amp; ask some questions.</p>