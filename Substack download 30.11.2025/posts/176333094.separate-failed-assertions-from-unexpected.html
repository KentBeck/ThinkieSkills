<p>When I implemented the first xUnit framework, without much thought I recorded failed assertions separately from unexpected exceptions—failures &amp; errors. Erich Gamma &amp; I copied this choice when we implemented JUnit. As part of a review of testing framework design (soon to appear as Canon JUnit), I got to thinking about this decision.</p><p>One thing I like about recording them separately is that unexpected exceptions tend to be easy to debug &amp; fix, while assertion failures tend to be more involved. Null pointer? Oh, something needed to be initialized. 75.4 instead of 68.3? Oh dear…</p><p>However, in some of my later bespoke testing frameworks I’ve lumped both flavors of failing tests together without losing productivity. So, what gives?</p><h2>Failure Density</h2><p>The difference is what I’m calling failure density. How many failures do you typically face at once?</p><p>At this point it shouldn’t surprise you that the number of failing tests per test run is power law distributed. From a preferential attachment perspective it makes sense—if something is going wrong it’s likely that other things are wrong too. However, that leaves us with alpha, the slope of the distribution.</p><p>High failure density: You sit down Monday morning. Pull from main. Run the tests. Twelve failures stare back at you. Maybe the team integrated a bunch of changes over the weekend. Maybe you were on vacation. Maybe someone pushed a breaking change to a shared library. Doesn’t matter—you’ve got twelve problems.</p><p>Low failure density: You write a test. It fails. You make it pass. You write another test. It fails. You make it pass. You’re never more than thirty seconds from green. One failure at a time, fixed immediately. Every every once in a while 2 tests will fail. Even 3. But mostly one or zero.</p><h2>Team Context</h2><p>I see high failure density in:</p><ul><li><p>Big teams (10+ developers) all committing to the same codebase</p></li><li><p>Teams with weak CI discipline—“we’ll fix the tests later”</p></li><li><p>Microservices where one team breaks another team’s tests</p></li><li><p>Monday mornings (the integration hangover)</p></li><li><p>Augmented coding when the genie runs ahead</p></li></ul><p>I see low failure density in:</p><ul><li><p>Solo developers or tight pairs</p></li><li><p>Strong TDD discipline—red, green, refactor, never leave red</p></li><li><p>Teams that stop everything when the build breaks</p></li><li><p>My own work (because I’ve learned to keep the batch small)</p></li></ul><h2>Separate?</h2><p>Back to the decision. With high failure density, separating exceptions from assertions helps. The twelve failures break into “8 NullPointerExceptions, 4 assertion failures.” The NPEs are probably from the same root cause—someone changed a constructor signature. Quick fix. Then you can focus on the four assertion failures, which need actual thinking.</p><p>With low failure density, separation is just ceremony. You have one broken test. Whether it’s an exception or an assertion doesn’t change what you do—you look at it and fix it.</p><p>I optimize for low failure density now. But I didn’t always. Early in my career, I worked on teams where high failure density was normal. Separating exceptions from assertions was genuinely helpful. Now I work differently. The separation would just be noise.</p><h2>And So…</h2><p>The framework decision depends on who you’re designing for. If your users typically face high failure density, give them categorization. If they’re in low failure density, don’t bother.​​​​​​​​​​​​​​​​</p>